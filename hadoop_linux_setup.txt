Hadoop Installation Linux
Setting up a Single Node Hadoop Cluster

Prerequisites to install Hadoop on Linux
Add a Hadoop system user

Pre-requisite
- Java 8

1. sudo apt install openjdk-8-jdk
2. sudo apt install openssh-server openssh-client -y
3. sudo adduser hdp
4. su - hdp
5. ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
6. cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
7. chmod 0600 ~/.ssh/authorized_keys
8. ssh localhost
9. wget hadoop_download_link
10. tar -xvf hadoop-3.3.3.tar.gz

11. sudo nano ~/.bashrc

Paste these commands at bottom of the file
export HADOOP_HOME=/home/hdp/hadoop-3.3.3
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"

======================================================
12. Save the bash file and close it
For applying all these changes to the current Terminal, execute the source command.

Command: source .bashrc

======================================================
Configurations
Command: cd hadoop-2.3.3/etc/hadoop/

Open core-site.xml and edit the property
Command: sudo nano core-site.xml

paste the xml code in file and save

<configuration>
<property>
<name>hadoop.tmp.dir</name>
<value>/app/hadoop/tmp</value>
<description>Parent directory for other temporary directories.</description>
</property>

<property>
<name>fs.defaultFS </name>
<value>hdfs://localhost:54310</value>
<description>The name of the default file system. </description>
</property>
</configuration>
======================================================

Edit file hdfs-site.xml
Command: sudo gedit hdfs-site.xml
paste xml code and save this file.

<configuration>
<property>
<name>dfs.replication</name>
<value>1</value>
</property>
<property>
<name>dfs.permission</name>
<value>false</value>
</property>
<property>
<name>dfs.data.dir</name>
<value>/home/hdp/hdfs/namenode</value>
</property>
<property>
<name>dfs.data.dir</name>
<value>/home/hdp/hdfs/datanode</value>
</property>
</configuration>
======================================================

Edit the mapred-site.xml file and edit the property
Command: sudo gedit mapred-site.xml

paste xml code and save this file.

<configuration>
<property>
<name>yarn.app.mapreduce.am.env</name>
  <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
</property>
<property>
  <name>mapreduce.map.env</name>
  <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
</property>
<property>
  <name>mapreduce.reduce.env</name>
  <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
</property>
</configuration>

======================================================

Edit file yarn-site.xml
Command: sudo gedit yarn-site.xml
paste xml code and save this file.

<configuration>
   <property>
    	<name>yarn.nodemanager.aux-services</name>
    	<value>mapreduce_shuffle</value>
   </property>
   <property>
	<name>yarn.nodemanager.auxservices.mapreduce.shuffle.class</name>  
	<value>org.apache.hadoop.mapred.ShuffleHandler</value>
   </property>
</configuration>
======================================================

Edit hadoop-env.sh and add the Java Path
Command: sudo gedit hadoopâ€“env.sh
export JAVA_HOME=/urs/lib/jvm/java-8-openjdk-amd64

======================================================
6. Go to Hadoop home directory and format the NameNode.
Command: cd
Command: cd hadoop-3.3.3
Command: sudo bin/hadoop namenode -format

======================================================
7. Testing
Go to hadoop-3.3.3/sbin directory and start all the daemons
Command: cd hadoop-3.3.3/sbin
Command: ./start-all.sh
The above command is a combination of start-dfs.sh, start-yarn.sh & mr-jobhistory-daemon.sh
======================================================
(Or you can start like this)
- Start namenode
Command: ./hadoop-daemon.sh start namenode
- Start Datanode
Command: ./hadoop-daemon.sh start datanode
- Start ResourceManager
Command: ./yarn-daemon.sh start resourcemanager
- Start NodeManager
Command: ./yarn-daemon.sh start nodemanager
======================================================
Start JobHistoryServer:
JobHistoryServer is responsible for servicing all job history related requests from client.

Command: ./mr-jobhistory-daemon.sh start historyserver
======================================================
8. To check that all the Hadoop services are up and running, run the below command.
Command: jps

Open: http://localhost:8088
Open: http://localhost:50070
======================================================
Hadoop installed Successfully............
======================================================
